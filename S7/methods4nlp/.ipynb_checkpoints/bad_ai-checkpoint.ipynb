{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9cae92",
   "metadata": {},
   "source": [
    "For this tutorial, we follow Robyn Speer's 2017 blog post [How to make a racist AI without really trying](http://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5f2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import re\n",
    "import statsmodels.formula.api\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ac1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure how graphs will show up in this notebook\n",
    "%matplotlib inline\n",
    "seaborn.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf78a8e",
   "metadata": {},
   "source": [
    "### Load the embeddings\n",
    "\n",
    "Download the embeddings from https://nlp.stanford.edu/data/glove.42B.300d.zip (we are using the 42B version here) and extract them into `data/glove.42B.300d.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03280d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "            print(\"done\")\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
    "\n",
    "embeddings = load_embeddings('data/glove.42B.300d.txt')\n",
    "embeddings.shape #(nb of vectors, length of vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdd79ca",
   "metadata": {},
   "source": [
    "### Load the lexicon\n",
    "\n",
    "Download the lexicon from Bing Liu’s web site (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon) and extract it into `data/positive-words.txt` and `data/negative-words.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(filename):\n",
    "    \"\"\"\n",
    "    Load a file from Bing Liu's sentiment lexicon\n",
    "    (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), containing\n",
    "    English words in Latin-1 encoding.\n",
    "    \n",
    "    One file contains a list of positive words, and the other contains\n",
    "    a list of negative words. The files contain comment lines starting\n",
    "    with ';' and blank lines, which should be skipped.\n",
    "    \"\"\"\n",
    "    lexicon = []\n",
    "    with open(filename, encoding='latin-1') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon\n",
    "\n",
    "pos_words = load_lexicon('data/positive-words.txt')\n",
    "neg_words = load_lexicon('data/negative-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095d581",
   "metadata": {},
   "source": [
    "### Train a model to predict word sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop words that are not in the Glove vocabulary (e.g. misspellings such a \"fancinating\") \n",
    "pos_vectors = embeddings.reindex(pos_words).dropna()\n",
    "neg_vectors = embeddings.reindex(neg_words).dropna()\n",
    "\n",
    "# Make arrays for the desired inputs and outputs:\n",
    "# the input are the embeddings, the outputs are -1 or 1\n",
    "# depending on whether a word is positive or negative\n",
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)\n",
    "\n",
    "# Split into test and training data (we use 10% for testing)\n",
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
    "\n",
    "# Make the classifier and train it. We run it for 100 iterations\n",
    "model = SGDClassifier(loss='log', random_state=0, max_iter=100)\n",
    "model.fit(train_vectors, train_targets)\n",
    "\n",
    "# Evaluate the classifier on the test vectors\n",
    "accuracy_score(model.predict(test_vectors), test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa7ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9c86da",
   "metadata": {},
   "source": [
    "### Word sentiment predictor functions\n",
    "\n",
    "We use these functions to see some examples of the model's predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecs_to_sentiment(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "\n",
    "def words_to_sentiment(words):\n",
    "    vecs = embeddings.reindex(words).dropna()\n",
    "    log_odds = vecs_to_sentiment(vecs)\n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c214b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show 20 examples from the test set\n",
    "words_to_sentiment(test_labels).iloc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55951f",
   "metadata": {},
   "source": [
    "### Get a sentiment score for text\n",
    "\n",
    "To get the sentiment score for a text, we are going to average the sentiment scores of its tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "# The regex above finds tokens that start with a word-like character (\\w), and continues\n",
    "# matching characters (.+?) until the next word break (\\b). It's a relatively simple\n",
    "# expression that manages to extract something very much like words from text.\n",
    "\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    tokens = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
    "    sentiments = words_to_sentiment(tokens)\n",
    "    return sentiments['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8033bd4e",
   "metadata": {},
   "source": [
    "Now let's test it on some sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sentiment(\"this example is pretty cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sentiment(\"this example is okay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e207941",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sentiment(\"meh, this example sucks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fe615",
   "metadata": {},
   "source": [
    "Now, let's try with a few more neutral sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f916fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sentiment(\"Let's go get Italian food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sentiment(\"Let's go get Chinese food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb62ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sentiment(\"Let's go get Mexican food\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a022f3ad",
   "metadata": {},
   "source": [
    "More neutral sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c776cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sentiment(\"My name is Emily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sentiment(\"My name is Heather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882abea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sentiment(\"My name is Yvette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_sentiment(\"My name is Shaniqua\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d0260",
   "metadata": {},
   "source": [
    "### Measuring the problem\n",
    "\n",
    "We use `ConceptNet`'s lists of names typically associated with specific ethnic backgrounds. Some notes on the origins of each list are provided in the comments above the lists by Robyn Speer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_BY_ETHNICITY = {\n",
    "    # The first two lists are from the Caliskan et al. appendix describing the\n",
    "    # Word Embedding Association Test.\n",
    "    'White': [\n",
    "        'Adam', 'Chip', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Ian', 'Justin',\n",
    "        'Ryan', 'Andrew', 'Fred', 'Jack', 'Matthew', 'Stephen', 'Brad', 'Greg', 'Jed',\n",
    "        'Paul', 'Todd', 'Brandon', 'Hank', 'Jonathan', 'Peter', 'Wilbur', 'Amanda',\n",
    "        'Courtney', 'Heather', 'Melanie', 'Sara', 'Amber', 'Crystal', 'Katie',\n",
    "        'Meredith', 'Shannon', 'Betsy', 'Donna', 'Kristin', 'Nancy', 'Stephanie',\n",
    "        'Bobbie-Sue', 'Ellen', 'Lauren', 'Peggy', 'Sue-Ellen', 'Colleen', 'Emily',\n",
    "        'Megan', 'Rachel', 'Wendy'\n",
    "    ],\n",
    "\n",
    "    'Black': [\n",
    "        'Alonzo', 'Jamel', 'Lerone', 'Percell', 'Theo', 'Alphonse', 'Jerome',\n",
    "        'Leroy', 'Rasaan', 'Torrance', 'Darnell', 'Lamar', 'Lionel', 'Rashaun',\n",
    "        'Tyree', 'Deion', 'Lamont', 'Malik', 'Terrence', 'Tyrone', 'Everol',\n",
    "        'Lavon', 'Marcellus', 'Terryl', 'Wardell', 'Aiesha', 'Lashelle', 'Nichelle',\n",
    "        'Shereen', 'Temeka', 'Ebony', 'Latisha', 'Shaniqua', 'Tameisha', 'Teretha',\n",
    "        'Jasmine', 'Latonya', 'Shanise', 'Tanisha', 'Tia', 'Lakisha', 'Latoya',\n",
    "        'Sharise', 'Tashika', 'Yolanda', 'Lashandra', 'Malika', 'Shavonn',\n",
    "        'Tawanda', 'Yvette'\n",
    "    ],\n",
    "    \n",
    "    # This list comes from statistics about common Hispanic-origin names in the US.\n",
    "    'Hispanic': [\n",
    "        'Juan', 'José', 'Miguel', 'Luís', 'Jorge', 'Santiago', 'Matías', 'Sebastián',\n",
    "        'Mateo', 'Nicolás', 'Alejandro', 'Samuel', 'Diego', 'Daniel', 'Tomás',\n",
    "        'Juana', 'Ana', 'Luisa', 'María', 'Elena', 'Sofía', 'Isabella', 'Valentina',\n",
    "        'Camila', 'Valeria', 'Ximena', 'Luciana', 'Mariana', 'Victoria', 'Martina'\n",
    "    ],\n",
    "    \n",
    "    # The following list conflates religion and ethnicity, I'm aware. So do given names.\n",
    "    #\n",
    "    # This list was cobbled together from searching baby-name sites for common Muslim names,\n",
    "    # as spelled in English. I did not ultimately distinguish whether the origin of the name\n",
    "    # is Arabic or Urdu or another language.\n",
    "    #\n",
    "    # I'd be happy to replace it with something more authoritative, given a source.\n",
    "    'Arab/Muslim': [\n",
    "        'Mohammed', 'Omar', 'Ahmed', 'Ali', 'Youssef', 'Abdullah', 'Yasin', 'Hamza',\n",
    "        'Ayaan', 'Syed', 'Rishaan', 'Samar', 'Ahmad', 'Zikri', 'Rayyan', 'Mariam',\n",
    "        'Jana', 'Malak', 'Salma', 'Nour', 'Lian', 'Fatima', 'Ayesha', 'Zahra', 'Sana',\n",
    "        'Zara', 'Alya', 'Shaista', 'Zoya', 'Yasmin'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff27d2",
   "metadata": {},
   "source": [
    "Now we use `pandas` to make a table of the names, their predominant ethnic background and the sentiment score we get for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070aa5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_sentiment_table():\n",
    "    frames = []\n",
    "    for group, name_list in sorted(NAMES_BY_ETHNICITY.items()):\n",
    "        lower_names = [name.lower() for name in name_list]\n",
    "        sentiments = words_to_sentiment(lower_names)\n",
    "        sentiments['group'] = group\n",
    "        frames.append(sentiments)\n",
    "\n",
    "    # Put together the data we got from each ethnic group into one big table\n",
    "    return pd.concat(frames)\n",
    "\n",
    "name_sentiments = name_sentiment_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73daeb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_sentiments.iloc[::25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e7da7",
   "metadata": {},
   "source": [
    "### Visualizations and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb55be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can visualize the distribution of sentiment we get for each kind of name\n",
    "plot = seaborn.swarmplot(x='group', y='sentiment', data=name_sentiments)\n",
    "plot.set_ylim([-10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d482e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also see that as a bar-plot, too, showing the 95% confidence intervals of the means\n",
    "plot = seaborn.barplot(x='group', y='sentiment', data=name_sentiments, capsize=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de97f69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we use the statsmodels package to see how big of an effect this is\n",
    "ols_model = statsmodels.formula.api.ols('sentiment ~ group', data=name_sentiments).fit()\n",
    "ols_model.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850eae1f",
   "metadata": {},
   "source": [
    "### Kings and queens and doctors and nurses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578786ab",
   "metadata": {},
   "source": [
    "We'll use a \"small\" set of embeddings pretrained with Glove and the `spacy` library for this next exercise. You can install `spacy` with `pip install spacy` or `pip3 install spacy` and then download the embeddings with\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_md\n",
    "```\n",
    "or\n",
    "\n",
    "```\n",
    "python3 -m spacy download en_core_web_md\n",
    "```\n",
    "depending on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcabfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.info()\n",
    "# Load the spacy model that you have downloaded\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(\"word embeddings loaded\")\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Define coside similarity\n",
    "cosine = lambda v1,v2: dot(v1,v2) / (norm(v1)*norm(v2))\n",
    "\n",
    "# Get the words that have vectors\n",
    "allwords = list({w for w in nlp.vocab if w.has_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c19b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(w1, w2, w3, n=10):\n",
    "    ''' Analogy function. Returns the n closest possibilities.\n",
    "    w1 is to w2 as w3 is to ... \n",
    "    e.g. \"Italy is to Rome as France is to...\" - result should be \"Paris\"\n",
    "    params: w1, w2, w3 - the first, second and third word in the statement respectively\n",
    "            n - how many words to return; default it 10\n",
    "    '''\n",
    "    w1_vector  = nlp(w1).vector\n",
    "    w2_vector = nlp(w2).vector\n",
    "    w3_vector   = nlp(w3).vector\n",
    "    x = w2_vector - w1_vector + w3_vector\n",
    "    allwords.sort(key=lambda w: cosine(w.vector,x))\n",
    "    allwords.reverse()\n",
    "    return [w.text for w in allwords[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_analogy(\"man\", \"doctor\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_analogy(\"italy\", \"rome\", \"france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42998616",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_analogy(\"man\", \"king\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_analogy(\"king\", \"man\", \"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98854635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
